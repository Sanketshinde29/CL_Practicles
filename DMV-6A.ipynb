{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0e9e0f-50b6-439d-8c70-110ca804c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045b034-9c94-4f31-b8cd-7b12cfd986ce",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a8af65-e4a7-4d7d-88b9-2e2b8841d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid dimensions\n",
    "grid_size = 5\n",
    "\n",
    "# Rewards: goal=10, penalty=-10, step=-1\n",
    "reward_matrix = -1 * np.ones((grid_size, grid_size))\n",
    "reward_matrix[4, 4] = 10  # Goal\n",
    "walls = [(1,1), (2,3), (3,1)]  # Wall positions\n",
    "for wall in walls:\n",
    "    reward_matrix[wall] = -10\n",
    "\n",
    "# Actions: Up, Down, Left, Right\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_map = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382f4d1-1771-4347-8ed1-d5fedeb35e2e",
   "metadata": {},
   "source": [
    "## Initialize Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5aa82fe-290e-4393-b20a-4dede32b1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table of shape (grid_size, grid_size, num_actions)\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bf697-186f-450f-8f01-d79694b9c2e5",
   "metadata": {},
   "source": [
    "## Define Helper Function to Take Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8d6aa1-d2bd-4f83-9815-ab4d41caafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state, action):\n",
    "    i, j = state\n",
    "    if action == 0:  # up\n",
    "        i = max(i-1, 0)\n",
    "    elif action == 1:  # down\n",
    "        i = min(i+1, grid_size-1)\n",
    "    elif action == 2:  # left\n",
    "        j = max(j-1, 0)\n",
    "    elif action == 3:  # right\n",
    "        j = min(j+1, grid_size-1)\n",
    "    \n",
    "    # Reward\n",
    "    reward = reward_matrix[i, j]\n",
    "    return (i, j), reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd769e73-0526-4e79-a09e-ac221776bcd0",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "152ec13d-690b-4ddd-bb42-4bb060a57f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1       # Learning rate\n",
    "gamma = 0.9       # Discount factor\n",
    "epsilon = 0.2     # Exploration rate\n",
    "episodes = 1000\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)  # Start state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        i, j = state\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 3)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[i,j,:])   # Exploit\n",
    "        \n",
    "        next_state, reward = take_action(state, action)\n",
    "        ni, nj = next_state\n",
    "        \n",
    "        # Update Q-value\n",
    "        Q[i,j,action] = Q[i,j,action] + alpha * (reward + gamma * np.max(Q[ni,nj,:]) - Q[i,j,action])\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if reward == 10 or reward == -10:\n",
    "            done = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479e27c-8c81-4883-b158-219807aba0b2",
   "metadata": {},
   "source": [
    "## Extract Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "363e8a64-1e3f-4872-84f8-765c45c061d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "[['d' 'r' 'd' 'r' 'd']\n",
      " ['d' 'W' 'd' 'r' 'd']\n",
      " ['r' 'r' 'd' 'W' 'd']\n",
      " ['d' 'W' 'd' 'd' 'd']\n",
      " ['r' 'r' 'r' 'r' 'G']]\n"
     ]
    }
   ],
   "source": [
    "policy = np.empty((grid_size, grid_size), dtype=str)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i,j) in walls:\n",
    "            policy[i,j] = 'WALL'\n",
    "        elif (i,j) == (4,4):\n",
    "            policy[i,j] = 'GOAL'\n",
    "        else:\n",
    "            policy[i,j] = action_map[np.argmax(Q[i,j,:])]\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceed4df-3d91-43e5-b817-c49f4a20e1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
